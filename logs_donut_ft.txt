nohup: zignorowane dane wejściowe
[cfg] base=naver-clova-ix/donut-base ds=katanaml-org/invoices-donut-data-v1 out=models/donut-auditor epochs=6 bs=1 ga=16 lr=5e-05 bf16=True fp16=False
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Traceback (most recent call last):
  File "/home/romaks/ai-audytor/scripts/train_donut_safe.py", line 65, in <module>
    args = TrainingArguments(
           ^^^^^^^^^^^^^^^^^^
TypeError: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
nohup: zignorowane dane wejściowe
[cfg] base=naver-clova-ix/donut-base ds=katanaml-org/invoices-donut-data-v1 out=models/donut-auditor epochs=6 bs=1 ga=16 lr=5e-05 bf16=True fp16=False
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Traceback (most recent call last):
  File "/home/romaks/ai-audytor/scripts/train_donut_safe.py", line 65, in <module>
    args = TrainingArguments(
           ^^^^^^^^^^^^^^^^^^
TypeError: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
nohup: zignorowane dane wejściowe
[cfg] base=naver-clova-ix/donut-base ds=katanaml-org/invoices-donut-data-v1 out=models/donut-auditor epochs=6 bs=1 ga=16 lr=5e-05 bf16=True fp16=False
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
/home/romaks/ai-audytor/scripts/train_donut_safe.py:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/162 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
  1%|          | 1/162 [00:07<19:40,  7.33s/it]  1%|          | 2/162 [00:13<17:24,  6.53s/it]  2%|▏         | 3/162 [00:19<16:34,  6.25s/it]  2%|▏         | 4/162 [00:25<15:58,  6.07s/it]  3%|▎         | 5/162 [00:30<15:46,  6.03s/it]  4%|▎         | 6/162 [00:36<15:33,  5.98s/it]  4%|▍         | 7/162 [00:42<15:27,  5.99s/it]  5%|▍         | 8/162 [00:48<15:14,  5.94s/it]  6%|▌         | 9/162 [00:54<15:04,  5.91s/it]  6%|▌         | 10/162 [01:00<14:56,  5.90s/it]  7%|▋         | 11/162 [01:06<14:48,  5.89s/it]  7%|▋         | 12/162 [01:12<14:40,  5.87s/it]  8%|▊         | 13/162 [01:18<14:40,  5.91s/it]  9%|▊         | 14/162 [01:24<14:37,  5.93s/it]  9%|▉         | 15/162 [01:29<14:26,  5.89s/it] 10%|▉         | 16/162 [01:35<14:17,  5.87s/it] 10%|█         | 17/162 [01:41<14:10,  5.87s/it] 11%|█         | 18/162 [01:47<14:10,  5.90s/it] 12%|█▏        | 19/162 [01:53<14:12,  5.96s/it] 12%|█▏        | 20/162 [01:59<14:12,  6.01s/it]                                                {'loss': 4.0865, 'grad_norm': 2.432616949081421, 'learning_rate': 4.413580246913581e-05, 'epoch': 0.75}
 12%|█▏        | 20/162 [01:59<14:12,  6.01s/it] 13%|█▎        | 21/162 [02:05<14:12,  6.04s/it] 14%|█▎        | 22/162 [02:12<14:12,  6.09s/it] 14%|█▍        | 23/162 [02:18<14:10,  6.12s/it] 15%|█▍        | 24/162 [02:24<14:01,  6.10s/it] 15%|█▌        | 25/162 [02:30<13:50,  6.06s/it] 16%|█▌        | 26/162 [02:36<13:44,  6.06s/it] 17%|█▋        | 27/162 [02:39<11:38,  5.17s/it] 17%|█▋        | 28/162 [02:46<12:40,  5.68s/it] 18%|█▊        | 29/162 [02:52<12:47,  5.77s/it] 19%|█▊        | 30/162 [02:58<12:47,  5.82s/it] 19%|█▉        | 31/162 [03:04<12:46,  5.85s/it] 20%|█▉        | 32/162 [03:09<12:35,  5.81s/it] 20%|██        | 33/162 [03:15<12:34,  5.85s/it] 21%|██        | 34/162 [03:21<12:28,  5.85s/it] 22%|██▏       | 35/162 [03:27<12:23,  5.85s/it] 22%|██▏       | 36/162 [03:33<12:17,  5.85s/it] 23%|██▎       | 37/162 [03:39<12:15,  5.89s/it] 23%|██▎       | 38/162 [03:45<12:06,  5.86s/it] 24%|██▍       | 39/162 [03:51<12:05,  5.90s/it] 25%|██▍       | 40/162 [03:56<11:56,  5.87s/it]                                                {'loss': 1.1432, 'grad_norm': 1.713139533996582, 'learning_rate': 3.7962962962962964e-05, 'epoch': 1.49}
 25%|██▍       | 40/162 [03:56<11:56,  5.87s/it] 25%|██▌       | 41/162 [04:02<11:50,  5.87s/it] 26%|██▌       | 42/162 [04:08<11:52,  5.94s/it] 27%|██▋       | 43/162 [04:14<11:51,  5.98s/it] 27%|██▋       | 44/162 [04:20<11:46,  5.99s/it] 28%|██▊       | 45/162 [04:26<11:39,  5.98s/it] 28%|██▊       | 46/162 [04:32<11:33,  5.98s/it] 29%|██▉       | 47/162 [04:38<11:24,  5.95s/it] 30%|██▉       | 48/162 [04:44<11:14,  5.92s/it] 30%|███       | 49/162 [04:50<11:04,  5.88s/it] 31%|███       | 50/162 [04:56<10:57,  5.87s/it] 31%|███▏      | 51/162 [05:02<10:49,  5.86s/it] 32%|███▏      | 52/162 [05:07<10:44,  5.86s/it] 33%|███▎      | 53/162 [05:13<10:40,  5.88s/it] 33%|███▎      | 54/162 [05:16<09:02,  5.02s/it] 34%|███▍      | 55/162 [05:23<09:49,  5.51s/it] 35%|███▍      | 56/162 [05:29<09:53,  5.60s/it] 35%|███▌      | 57/162 [05:35<09:53,  5.65s/it] 36%|███▌      | 58/162 [05:40<09:52,  5.69s/it] 36%|███▋      | 59/162 [05:46<09:48,  5.71s/it] 37%|███▋      | 60/162 [05:52<09:44,  5.73s/it]                                                {'loss': 0.4834, 'grad_norm': 1.3482540845870972, 'learning_rate': 3.1790123456790125e-05, 'epoch': 2.23}
 37%|███▋      | 60/162 [05:52<09:44,  5.73s/it] 38%|███▊      | 61/162 [05:58<09:40,  5.75s/it] 38%|███▊      | 62/162 [06:04<09:35,  5.76s/it] 39%|███▉      | 63/162 [06:09<09:36,  5.83s/it] 40%|███▉      | 64/162 [06:15<09:31,  5.83s/it] 40%|████      | 65/162 [06:21<09:29,  5.87s/it] 41%|████      | 66/162 [06:27<09:27,  5.92s/it] 41%|████▏     | 67/162 [06:33<09:23,  5.93s/it] 42%|████▏     | 68/162 [06:39<09:12,  5.88s/it] 43%|████▎     | 69/162 [06:45<09:07,  5.89s/it] 43%|████▎     | 70/162 [06:51<09:02,  5.90s/it] 44%|████▍     | 71/162 [06:57<08:54,  5.88s/it] 44%|████▍     | 72/162 [07:03<08:51,  5.90s/it] 45%|████▌     | 73/162 [07:09<08:46,  5.91s/it] 46%|████▌     | 74/162 [07:14<08:39,  5.90s/it] 46%|████▋     | 75/162 [07:20<08:34,  5.91s/it]nohup: zignorowane dane wejściowe
[cfg] base=naver-clova-ix/donut-base ds=katanaml-org/invoices-donut-data-v1 out=models/donut-auditor epochs=6 bs=1 ga=16 lr=5e-05 bf16=True fp16=False
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
/home/romaks/ai-audytor/scripts/train_donut_safe.py:82: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|          | 0/162 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
  1%|          | 1/162 [00:06<18:31,  6.90s/it]  1%|          | 2/162 [00:12<16:31,  6.20s/it]  2%|▏         | 3/162 [00:18<15:46,  5.95s/it]  2%|▏         | 4/162 [00:23<15:21,  5.83s/it]  3%|▎         | 5/162 [00:29<15:07,  5.78s/it]  4%|▎         | 6/162 [00:35<14:53,  5.73s/it]  4%|▍         | 7/162 [00:40<14:49,  5.74s/it]  5%|▍         | 8/162 [00:46<14:48,  5.77s/it]  6%|▌         | 9/162 [00:52<14:43,  5.78s/it]  6%|▌         | 10/162 [00:58<14:44,  5.82s/it]  7%|▋         | 11/162 [01:04<14:34,  5.79s/it]  7%|▋         | 12/162 [01:09<14:22,  5.75s/it]  8%|▊         | 13/162 [01:15<14:13,  5.73s/it]  9%|▊         | 14/162 [01:21<14:03,  5.70s/it]  9%|▉         | 15/162 [01:26<13:56,  5.69s/it] 10%|▉         | 16/162 [01:32<13:49,  5.68s/it] 10%|█         | 17/162 [01:38<13:43,  5.68s/it] 11%|█         | 18/162 [01:43<13:36,  5.67s/it] 12%|█▏        | 19/162 [01:49<13:30,  5.67s/it] 12%|█▏        | 20/162 [01:55<13:27,  5.69s/it]                                                {'loss': 4.0914, 'grad_norm': 2.5666422843933105, 'learning_rate': 4.413580246913581e-05, 'epoch': 0.75}
 12%|█▏        | 20/162 [01:55<13:27,  5.69s/it] 13%|█▎        | 21/162 [02:01<13:23,  5.70s/it] 14%|█▎        | 22/162 [02:06<13:17,  5.70s/it] 14%|█▍        | 23/162 [02:12<13:11,  5.69s/it] 15%|█▍        | 24/162 [02:18<13:13,  5.75s/it] 15%|█▌        | 25/162 [02:24<13:12,  5.78s/it] 16%|█▌        | 26/162 [02:29<13:04,  5.77s/it] 17%|█▋        | 27/162 [02:32<11:03,  4.92s/it] 17%|█▋        | 28/162 [02:39<12:03,  5.40s/it] 18%|█▊        | 29/162 [02:45<12:11,  5.50s/it] 19%|█▊        | 30/162 [02:50<12:19,  5.61s/it] 19%|█▉        | 31/162 [02:56<12:32,  5.75s/it] 20%|█▉        | 32/162 [03:02<12:37,  5.83s/it] 20%|██        | 33/162 [03:08<12:31,  5.82s/it] 21%|██        | 34/162 [03:14<12:28,  5.85s/it] 22%|██▏       | 35/162 [03:20<12:23,  5.86s/it] 22%|██▏       | 36/162 [03:26<12:16,  5.84s/it] 23%|██▎       | 37/162 [03:32<12:08,  5.83s/it] 23%|██▎       | 38/162 [03:38<12:04,  5.84s/it] 24%|██▍       | 39/162 [03:43<11:57,  5.84s/it] 25%|██▍       | 40/162 [03:49<11:52,  5.84s/it]                                                {'loss': 1.1532, 'grad_norm': 1.8012340068817139, 'learning_rate': 3.7962962962962964e-05, 'epoch': 1.49}
 25%|██▍       | 40/162 [03:49<11:52,  5.84s/it] 25%|██▌       | 41/162 [03:55<11:49,  5.86s/it] 26%|██▌       | 42/162 [04:01<11:38,  5.82s/it] 27%|██▋       | 43/162 [04:07<11:37,  5.86s/it] 27%|██▋       | 44/162 [04:13<11:34,  5.89s/it] 28%|██▊       | 45/162 [04:19<11:24,  5.85s/it] 28%|██▊       | 46/162 [04:24<11:16,  5.83s/it] 29%|██▉       | 47/162 [04:30<11:09,  5.82s/it] 30%|██▉       | 48/162 [04:36<11:04,  5.83s/it] 30%|███       | 49/162 [04:42<10:56,  5.81s/it] 31%|███       | 50/162 [04:48<10:50,  5.81s/it] 31%|███▏      | 51/162 [04:53<10:44,  5.81s/it] 32%|███▏      | 52/162 [04:59<10:38,  5.80s/it] 33%|███▎      | 53/162 [05:05<10:33,  5.81s/it] 33%|███▎      | 54/162 [05:08<08:58,  4.98s/it] 34%|███▍      | 55/162 [05:15<09:46,  5.49s/it] 35%|███▍      | 56/162 [05:21<09:52,  5.59s/it] 35%|███▌      | 57/162 [05:26<09:55,  5.68s/it] 36%|███▌      | 58/162 [05:32<09:54,  5.71s/it] 36%|███▋      | 59/162 [05:38<09:51,  5.75s/it] 37%|███▋      | 60/162 [05:44<09:48,  5.77s/it]                                                {'loss': 0.4846, 'grad_norm': 1.3570243120193481, 'learning_rate': 3.1790123456790125e-05, 'epoch': 2.23}
 37%|███▋      | 60/162 [05:44<09:48,  5.77s/it] 38%|███▊      | 61/162 [05:50<09:46,  5.81s/it] 38%|███▊      | 62/162 [05:56<09:40,  5.80s/it] 39%|███▉      | 63/162 [06:01<09:33,  5.79s/it] 40%|███▉      | 64/162 [06:07<09:28,  5.80s/it] 40%|████      | 65/162 [06:13<09:29,  5.87s/it] 41%|████      | 66/162 [06:19<09:24,  5.88s/it] 41%|████▏     | 67/162 [06:25<09:19,  5.89s/it] 42%|████▏     | 68/162 [06:31<09:16,  5.92s/it] 43%|████▎     | 69/162 [06:37<09:08,  5.90s/it] 43%|████▎     | 70/162 [06:43<09:00,  5.87s/it] 44%|████▍     | 71/162 [06:48<08:53,  5.87s/it] 44%|████▍     | 72/162 [06:54<08:44,  5.82s/it] 45%|████▌     | 73/162 [07:00<08:38,  5.82s/it] 46%|████▌     | 74/162 [07:06<08:36,  5.87s/it] 46%|████▋     | 75/162 [07:12<08:32,  5.90s/it] 47%|████▋     | 76/162 [07:18<08:23,  5.86s/it] 48%|████▊     | 77/162 [07:23<08:15,  5.83s/it] 48%|████▊     | 78/162 [07:29<08:10,  5.84s/it] 49%|████▉     | 79/162 [07:35<08:03,  5.83s/it] 49%|████▉     | 80/162 [07:41<07:58,  5.83s/it]                                                {'loss': 0.2996, 'grad_norm': 1.6504911184310913, 'learning_rate': 2.5617283950617287e-05, 'epoch': 2.98}
 49%|████▉     | 80/162 [07:41<07:58,  5.83s/it] 50%|█████     | 81/162 [07:44<06:43,  4.98s/it] 51%|█████     | 82/162 [07:51<07:17,  5.47s/it] 51%|█████     | 83/162 [07:56<07:22,  5.60s/it] 52%|█████▏    | 84/162 [08:02<07:24,  5.70s/it] 52%|█████▏    | 85/162 [08:08<07:22,  5.74s/it] 53%|█████▎    | 86/162 [08:14<07:18,  5.77s/it] 54%|█████▎    | 87/162 [08:20<07:12,  5.77s/it] 54%|█████▍    | 88/162 [08:26<07:06,  5.76s/it] 55%|█████▍    | 89/162 [08:32<07:04,  5.81s/it] 56%|█████▌    | 90/162 [08:37<07:00,  5.85s/it] 56%|█████▌    | 91/162 [08:43<06:56,  5.87s/it] 57%|█████▋    | 92/162 [08:49<06:48,  5.84s/it] 57%|█████▋    | 93/162 [08:55<06:41,  5.82s/it] 58%|█████▊    | 94/162 [09:01<06:34,  5.80s/it] 59%|█████▊    | 95/162 [09:07<06:31,  5.85s/it] 59%|█████▉    | 96/162 [09:13<06:29,  5.90s/it] 60%|█████▉    | 97/162 [09:19<06:25,  5.93s/it] 60%|██████    | 98/162 [09:25<06:18,  5.92s/it] 61%|██████    | 99/162 [09:31<06:15,  5.95s/it] 62%|██████▏   | 100/162 [09:37<06:11,  6.00s/it]                                                 {'loss': 0.2235, 'grad_norm': 0.8546016216278076, 'learning_rate': 1.9444444444444445e-05, 'epoch': 3.72}
 62%|██████▏   | 100/162 [09:37<06:11,  6.00s/it] 62%|██████▏   | 101/162 [09:43<06:05,  5.99s/it] 63%|██████▎   | 102/162 [09:49<05:59,  5.99s/it] 64%|██████▎   | 103/162 [09:55<05:52,  5.97s/it] 64%|██████▍   | 104/162 [10:00<05:42,  5.91s/it] 65%|██████▍   | 105/162 [10:06<05:34,  5.87s/it] 65%|██████▌   | 106/162 [10:12<05:26,  5.83s/it] 66%|██████▌   | 107/162 [10:18<05:18,  5.80s/it] 67%|██████▋   | 108/162 [10:21<04:28,  4.98s/it] 67%|██████▋   | 109/162 [10:27<04:50,  5.49s/it] 68%|██████▊   | 110/162 [10:33<04:50,  5.58s/it] 69%|██████▊   | 111/162 [10:39<04:50,  5.70s/it] 69%|██████▉   | 112/162 [10:45<04:46,  5.73s/it] 70%|██████▉   | 113/162 [10:51<04:41,  5.75s/it] 70%|███████   | 114/162 [10:56<04:36,  5.75s/it] 71%|███████   | 115/162 [11:02<04:31,  5.77s/it] 72%|███████▏  | 116/162 [11:08<04:26,  5.80s/it] 72%|███████▏  | 117/162 [11:14<04:24,  5.88s/it] 73%|███████▎  | 118/162 [11:20<04:21,  5.94s/it] 73%|███████▎  | 119/162 [11:26<04:16,  5.98s/it] 74%|███████▍  | 120/162 [11:32<04:11,  5.98s/it]                                                 {'loss': 0.183, 'grad_norm': 0.6997044682502747, 'learning_rate': 1.3271604938271605e-05, 'epoch': 4.45}
 74%|███████▍  | 120/162 [11:32<04:11,  5.98s/it] 75%|███████▍  | 121/162 [11:38<04:04,  5.97s/it] 75%|███████▌  | 122/162 [11:44<03:58,  5.96s/it] 76%|███████▌  | 123/162 [11:50<03:51,  5.95s/it] 77%|███████▋  | 124/162 [11:56<03:43,  5.89s/it] 77%|███████▋  | 125/162 [12:02<03:36,  5.85s/it] 78%|███████▊  | 126/162 [12:07<03:30,  5.85s/it] 78%|███████▊  | 127/162 [12:13<03:24,  5.84s/it] 79%|███████▉  | 128/162 [12:19<03:17,  5.82s/it] 80%|███████▉  | 129/162 [12:25<03:12,  5.82s/it] 80%|████████  | 130/162 [12:31<03:06,  5.84s/it] 81%|████████  | 131/162 [12:37<03:01,  5.84s/it] 81%|████████▏ | 132/162 [12:42<02:54,  5.83s/it] 82%|████████▏ | 133/162 [12:48<02:48,  5.81s/it] 83%|████████▎ | 134/162 [12:54<02:42,  5.79s/it] 83%|████████▎ | 135/162 [12:57<02:13,  4.96s/it] 84%|████████▍ | 136/162 [13:04<02:23,  5.50s/it] 85%|████████▍ | 137/162 [13:10<02:20,  5.61s/it] 85%|████████▌ | 138/162 [13:15<02:16,  5.69s/it] 86%|████████▌ | 139/162 [13:21<02:10,  5.69s/it] 86%|████████▋ | 140/162 [13:27<02:06,  5.77s/it]                                                 {'loss': 0.1612, 'grad_norm': 0.826181948184967, 'learning_rate': 7.098765432098765e-06, 'epoch': 5.19}
 86%|████████▋ | 140/162 [13:27<02:06,  5.77s/it] 87%|████████▋ | 141/162 [13:33<02:02,  5.85s/it] 88%|████████▊ | 142/162 [13:39<01:56,  5.83s/it] 88%|████████▊ | 143/162 [13:45<01:50,  5.83s/it] 89%|████████▉ | 144/162 [13:51<01:45,  5.85s/it] 90%|████████▉ | 145/162 [13:57<01:39,  5.86s/it] 90%|█████████ | 146/162 [14:02<01:33,  5.83s/it] 91%|█████████ | 147/162 [14:08<01:27,  5.83s/it] 91%|█████████▏| 148/162 [14:14<01:21,  5.81s/it] 92%|█████████▏| 149/162 [14:20<01:15,  5.81s/it] 93%|█████████▎| 150/162 [14:26<01:10,  5.84s/it] 93%|█████████▎| 151/162 [14:31<01:04,  5.84s/it] 94%|█████████▍| 152/162 [14:37<00:58,  5.85s/it] 94%|█████████▍| 153/162 [14:43<00:52,  5.83s/it] 95%|█████████▌| 154/162 [14:49<00:46,  5.85s/it] 96%|█████████▌| 155/162 [14:55<00:40,  5.83s/it] 96%|█████████▋| 156/162 [15:01<00:34,  5.83s/it] 97%|█████████▋| 157/162 [15:07<00:29,  5.85s/it] 98%|█████████▊| 158/162 [15:12<00:23,  5.85s/it] 98%|█████████▊| 159/162 [15:18<00:17,  5.88s/it] 99%|█████████▉| 160/162 [15:24<00:11,  5.91s/it]                                                 {'loss': 0.1489, 'grad_norm': 0.6292781233787537, 'learning_rate': 9.259259259259259e-07, 'epoch': 5.94}
 99%|█████████▉| 160/162 [15:24<00:11,  5.91s/it] 99%|█████████▉| 161/162 [15:30<00:05,  5.89s/it]100%|██████████| 162/162 [15:33<00:00,  5.04s/it]/home/romaks/ai-audytor/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3909: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 768}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
                                                 {'train_runtime': 935.7242, 'train_samples_per_second': 2.725, 'train_steps_per_second': 0.173, 'train_loss': 0.834527611364553, 'epoch': 6.0}
100%|██████████| 162/162 [15:35<00:00,  5.04s/it]100%|██████████| 162/162 [15:35<00:00,  5.78s/it]
[done] saved to models/donut-auditor
nohup: zignorowane dane wejściowe
[cfg] base=naver-clova-ix/donut-base ds=katanaml-org/invoices-donut-data-v1 out=models/donut-auditor epochs=6 bs=1 ga=16 lr=5e-05 bf16=True fp16=False
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
/home/romaks/ai-audytor/scripts/train_donut_safe.py:82: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
There were missing keys in the checkpoint model loaded: ['decoder.lm_head.weight'].
  0%|          | 0/162 [00:00<?, ?it/s]                                       {'train_runtime': 0.0055, 'train_samples_per_second': 462107.375, 'train_steps_per_second': 29357.41, 'train_loss': 0.0, 'epoch': 6.0}
  0%|          | 0/162 [00:00<?, ?it/s]  0%|          | 0/162 [00:00<?, ?it/s]
/home/romaks/ai-audytor/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3909: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 768}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
[done] saved to models/donut-auditor
[cfg] base=naver-clova-ix/donut-base ds=katanaml-org/invoices-donut-data-v1 out=models/donut-auditor epochs=6 bs=1 ga=16 lr=5e-05 bf16=True fp16=False
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
/home/romaks/ai-audytor/scripts/train_donut_safe.py:82: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
There were missing keys in the checkpoint model loaded: ['decoder.lm_head.weight'].
  0%|          | 0/162 [00:00<?, ?it/s]                                       {'train_runtime': 0.0044, 'train_samples_per_second': 577228.949, 'train_steps_per_second': 36671.016, 'train_loss': 0.0, 'epoch': 6.0}
  0%|          | 0/162 [00:00<?, ?it/s]  0%|          | 0/162 [00:00<?, ?it/s]
/home/romaks/ai-audytor/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3909: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 768}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
  warnings.warn(
[done] saved to models/donut-auditor
nohup: zignorowane dane wejściowe
[cfg] base=naver-clova-ix/donut-base ds=katanaml-org/invoices-donut-data-v1 out=models/donut-auditor epochs=6 bs=1 ga=16 lr=5e-05 bf16=True fp16=False
Map:   0%|          | 0/425 [00:00<?, ? examples/s]Map:   0%|          | 0/425 [01:22<?, ? examples/s]Map:   0%|          | 0/425 [01:22<?, ? examples/s]
Traceback (most recent call last):
  File "/home/romaks/ai-audytor/scripts/train_donut_safe.py", line 37, in <module>
    train = ds["train"].map(encode_batch, batched=True, remove_columns=ds["train"].column_names)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/romaks/ai-audytor/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 560, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/romaks/ai-audytor/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3318, in map
    for rank, done, content in Dataset._map_single(**unprocessed_kwargs):
  File "/home/romaks/ai-audytor/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 3689, in _map_single
    writer.write_batch(batch, try_original_type=try_original_type)
  File "/home/romaks/ai-audytor/.venv/lib/python3.12/site-packages/datasets/arrow_writer.py", line 631, in write_batch
    self.write_table(pa_table, writer_batch_size)
  File "/home/romaks/ai-audytor/.venv/lib/python3.12/site-packages/datasets/arrow_writer.py", line 643, in write_table
    pa_table = pa_table.combine_chunks()
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/table.pxi", line 4557, in pyarrow.lib.Table.combine_chunks
  File "pyarrow/error.pxi", line 155, in pyarrow.lib.pyarrow_internal_check_status
  File "pyarrow/error.pxi", line 92, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays, consider casting input from `list<item: list<item: list<item: float>>>` to `list<item: list<item: large_list<item: float>>>` first.
