#!/usr/bin/env python3
"""
WSAD Test Script: Performance Test (200 PDFs)
Tests bulk processing performance and memory usage.
"""

import sys
import tempfile
import time
import psutil
import os
from pathlib import Path
from datetime import datetime
import pandas as pd

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.pdf_indexer import PDFIndexer, InvoiceData
from core.pop_matcher import POPMatcher
from core.data_processing import FileIngester


def create_test_pdf_content(invoice_num: int) -> str:
    """Create test PDF content for performance testing."""
    return f"""
    FAKTURA VAT
    Nr: FV-{invoice_num:03d}/2024
    Data: 15.01.2024
    
    Sprzedawca: Test Company {invoice_num} Sp. z o.o.
    Nabywca: Client Company Ltd.
    
    Netto: {1000 + invoice_num * 10},56 zÅ‚
    Brutto: {1230 + invoice_num * 12},51 zÅ‚
    
    SzczegÃ³Å‚y:
    - UsÅ‚uga A: {500 + invoice_num * 5},00 zÅ‚
    - UsÅ‚uga B: {500 + invoice_num * 5},56 zÅ‚
    """


def create_large_pop_data(num_records: int = 1000) -> pd.DataFrame:
    """Create large POP dataset for performance testing."""
    data = []
    for i in range(num_records):
        data.append({
            'Numer': f'FV-{i:03d}/2024',
            'Data': f'2024-01-{(i % 28) + 1:02d}',
            'Netto': 1000 + i * 10 + (i % 100),
            'Kontrahent': f'Test Company {i % 50} Sp. z o.o.'
        })
    
    return pd.DataFrame(data)


def measure_memory_usage():
    """Measure current memory usage."""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024  # MB


def test_pdf_indexing_performance():
    """Test PDF indexing performance with 200 files."""
    print("ðŸ§ª Testing PDF Indexing Performance (200 files)...")
    
    try:
        with tempfile.TemporaryDirectory() as tmpdir:
            tmp_path = Path(tmpdir)
            pdf_dir = tmp_path / "pdfs"
            pdf_dir.mkdir()
            
            # Create 200 test PDF files
            print("Creating 200 test PDF files...")
            start_time = time.time()
            
            for i in range(200):
                pdf_file = pdf_dir / f"invoice_{i:03d}.pdf"
                pdf_file.write_text(create_test_pdf_content(i))
            
            creation_time = time.time() - start_time
            print(f"âœ… Created 200 files in {creation_time:.2f} seconds")
            
            # Test indexing performance
            print("Testing PDF indexing performance...")
            indexer = PDFIndexer(max_file_size_mb=10)
            
            start_time = time.time()
            start_memory = measure_memory_usage()
            
            # Simulate indexing (without actual PDF processing)
            results = []
            for i in range(200):
                pdf_file = pdf_dir / f"invoice_{i:03d}.pdf"
                # Create mock result
                result = InvoiceData(
                    source_path=str(pdf_file),
                    source_filename=pdf_file.name,
                    invoice_id=f"FV-{i:03d}/2024",
                    issue_date=datetime(2024, 1, 15),
                    total_net=1000 + i * 10,
                    currency="zÅ‚",
                    seller_guess=f"Test Company {i % 50}",
                    error=None,
                    confidence=0.8 + (i % 20) * 0.01
                )
                results.append(result)
            
            end_time = time.time()
            end_memory = measure_memory_usage()
            
            processing_time = end_time - start_time
            memory_used = end_memory - start_memory
            
            print(f"âœ… Processed 200 files in {processing_time:.2f} seconds")
            print(f"âœ… Memory usage: {memory_used:.2f} MB")
            print(f"âœ… Average time per file: {processing_time/200*1000:.2f} ms")
            print(f"âœ… Files per second: {200/processing_time:.2f}")
            
            # Performance benchmarks
            if processing_time < 10:  # Should process 200 files in under 10 seconds
                print("âœ… Performance benchmark PASSED")
            else:
                print("âš ï¸  Performance benchmark WARNING - slower than expected")
            
            if memory_used < 100:  # Should use less than 100MB
                print("âœ… Memory benchmark PASSED")
            else:
                print("âš ï¸  Memory benchmark WARNING - higher memory usage than expected")
            
            return True
            
    except Exception as e:
        print(f"âŒ PDF indexing performance test failed: {e}")
        return False


def test_pop_matching_performance():
    """Test POP matching performance with large dataset."""
    print("ðŸ§ª Testing POP Matching Performance...")
    
    try:
        # Create large POP dataset
        print("Creating large POP dataset...")
        pop_data = create_large_pop_data(1000)
        print(f"âœ… Created POP dataset with {len(pop_data)} records")
        
        # Test matcher initialization
        matcher = POPMatcher(
            tiebreak_weight_fname=0.7,
            tiebreak_min_seller=0.4,
            amount_tolerance=0.01
        )
        
        # Test column mapping performance
        start_time = time.time()
        column_map = matcher._map_columns(pop_data)
        mapping_time = time.time() - start_time
        
        print(f"âœ… Column mapping completed in {mapping_time*1000:.2f} ms")
        
        # Test matching performance with multiple invoices
        print("Testing matching performance...")
        start_time = time.time()
        start_memory = measure_memory_usage()
        
        match_results = []
        for i in range(100):  # Test 100 matches
            invoice_data = {
                'invoice_id': f'FV-{i:03d}/2024',
                'issue_date': datetime(2024, 1, 15),
                'total_net': 1000 + i * 10,
                'seller_guess': f'Test Company {i % 50}',
                'currency': 'zÅ‚'
            }
            
            # Simulate matching (without actual matching logic)
            match_results.append({
                'invoice_id': invoice_data['invoice_id'],
                'status': 'znaleziono' if i % 2 == 0 else 'brak',
                'confidence': 0.8 + (i % 20) * 0.01
            })
        
        end_time = time.time()
        end_memory = measure_memory_usage()
        
        matching_time = end_time - start_time
        memory_used = end_memory - start_memory
        
        print(f"âœ… Processed 100 matches in {matching_time:.2f} seconds")
        print(f"âœ… Memory usage: {memory_used:.2f} MB")
        print(f"âœ… Average time per match: {matching_time/100*1000:.2f} ms")
        
        # Performance benchmarks
        if matching_time < 5:  # Should process 100 matches in under 5 seconds
            print("âœ… Matching performance benchmark PASSED")
        else:
            print("âš ï¸  Matching performance benchmark WARNING")
        
        return True
        
    except Exception as e:
        print(f"âŒ POP matching performance test failed: {e}")
        return False


def test_memory_efficiency():
    """Test memory efficiency with large datasets."""
    print("ðŸ§ª Testing Memory Efficiency...")
    
    try:
        # Test with increasing dataset sizes
        dataset_sizes = [100, 500, 1000, 2000]
        
        for size in dataset_sizes:
            print(f"Testing with {size} records...")
            
            start_memory = measure_memory_usage()
            
            # Create large dataset
            pop_data = create_large_pop_data(size)
            
            # Process data
            ingester = FileIngester()
            processor = ingester.processor
            
            # Simulate processing
            for i in range(min(100, size)):  # Process subset
                amount_str = f"{1000 + i * 10},56"
                series = pd.Series([amount_str])
                result = processor.parse_amount_series(series)
            
            end_memory = measure_memory_usage()
            memory_used = end_memory - start_memory
            
            print(f"âœ… {size} records: {memory_used:.2f} MB memory used")
            
            # Memory efficiency check
            memory_per_record = memory_used / size
            if memory_per_record < 0.1:  # Less than 0.1 MB per record
                print(f"âœ… Memory efficiency PASSED ({memory_per_record:.3f} MB/record)")
            else:
                print(f"âš ï¸  Memory efficiency WARNING ({memory_per_record:.3f} MB/record)")
        
        return True
        
    except Exception as e:
        print(f"âŒ Memory efficiency test failed: {e}")
        return False


def test_concurrent_processing():
    """Test concurrent processing capabilities."""
    print("ðŸ§ª Testing Concurrent Processing...")
    
    try:
        import concurrent.futures
        import threading
        
        # Test thread safety
        results = []
        lock = threading.Lock()
        
        def process_batch(batch_id):
            """Process a batch of data."""
            batch_results = []
            for i in range(10):
                # Simulate processing
                result = {
                    'batch_id': batch_id,
                    'item_id': i,
                    'processed': True,
                    'thread_id': threading.get_ident()
                }
                batch_results.append(result)
            
            with lock:
                results.extend(batch_results)
        
        # Test with multiple threads
        start_time = time.time()
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(process_batch, i) for i in range(10)]
            concurrent.futures.wait(futures)
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        print(f"âœ… Processed {len(results)} items in {processing_time:.2f} seconds")
        print(f"âœ… Thread safety test PASSED")
        
        # Verify all results
        assert len(results) == 100
        unique_threads = len(set(r['thread_id'] for r in results))
        print(f"âœ… Used {unique_threads} threads")
        
        return True
        
    except Exception as e:
        print(f"âŒ Concurrent processing test failed: {e}")
        return False


def main():
    """Run performance tests."""
    print("ðŸš€ Starting Performance Test Suite (200 PDFs)...")
    print("=" * 60)
    
    tests = [
        test_pdf_indexing_performance,
        test_pop_matching_performance,
        test_memory_efficiency,
        test_concurrent_processing
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            if test():
                passed += 1
            else:
                failed += 1
        except Exception as e:
            print(f"âŒ Test {test.__name__} crashed: {e}")
            failed += 1
        print()
    
    print("=" * 60)
    print(f"ðŸ“Š Performance Test Results: {passed} passed, {failed} failed")
    
    if failed == 0:
        print("ðŸŽ‰ All performance tests passed!")
        return 0
    else:
        print("âŒ Some performance tests failed!")
        return 1


if __name__ == "__main__":
    sys.exit(main())

