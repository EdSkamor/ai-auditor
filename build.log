Using pip 25.2 from /home/romaks/ai-audytor/.venv/lib/python3.12/site-packages/pip (python 3.12)
Collecting llama-cpp-python
  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.7/50.7 MB 47.6 MB/s  0:00:01
  Installing build dependencies: started
  Running command pip subprocess to install build dependencies
  Using pip 25.2 from /home/romaks/ai-audytor/.venv/lib/python3.12/site-packages/pip (python 3.12)
  Collecting scikit-build-core>=0.9.2 (from scikit-build-core[pyproject]>=0.9.2)
    Obtaining dependency information for scikit-build-core>=0.9.2 from https://files.pythonhosted.org/packages/43/49/ec16b3db6893db788ae35f98506ff5a9c25dca7eb18cc38ada8a4c1dc944/scikit_build_core-0.11.6-py3-none-any.whl.metadata
    Downloading scikit_build_core-0.11.6-py3-none-any.whl.metadata (18 kB)
  Collecting packaging>=23.2 (from scikit-build-core>=0.9.2->scikit-build-core[pyproject]>=0.9.2)
    Obtaining dependency information for packaging>=23.2 from https://files.pythonhosted.org/packages/20/12/38679034af332785aac8774540895e234f4d07f7545804097de4b666afd8/packaging-25.0-py3-none-any.whl.metadata
    Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)
  Collecting pathspec>=0.10.1 (from scikit-build-core>=0.9.2->scikit-build-core[pyproject]>=0.9.2)
    Obtaining dependency information for pathspec>=0.10.1 from https://files.pythonhosted.org/packages/cc/20/ff623b09d963f88bfde16306a54e12ee5ea43e9b597108672ff3a408aad6/pathspec-0.12.1-py3-none-any.whl.metadata
    Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)
  Downloading scikit_build_core-0.11.6-py3-none-any.whl (185 kB)
  Using cached packaging-25.0-py3-none-any.whl (66 kB)
  Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)
  Installing collected packages: pathspec, packaging, scikit-build-core

  Successfully installed packaging-25.0 pathspec-0.12.1 scikit-build-core-0.11.6
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Running command Getting requirements to build wheel
  Getting requirements to build wheel: finished with status 'done'
  Installing backend dependencies: started
  Running command pip subprocess to install backend dependencies
  Using pip 25.2 from /home/romaks/ai-audytor/.venv/lib/python3.12/site-packages/pip (python 3.12)
  Collecting ninja>=1.5
    Obtaining dependency information for ninja>=1.5 from https://files.pythonhosted.org/packages/ed/de/0e6edf44d6a04dabd0318a519125ed0415ce437ad5a1ec9b9be03d9048cf/ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata
    Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)
  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)
  Installing collected packages: ninja
  Successfully installed ninja-1.13.0
  Installing backend dependencies: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Running command Preparing metadata (pyproject.toml)
  *** scikit-build-core 0.11.6 using CMake 3.28.3 (metadata_wheel)
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.12/site-packages (from llama-cpp-python) (4.14.1)
Requirement already satisfied: numpy>=1.20.0 in ./.venv/lib/python3.12/site-packages (from llama-cpp-python) (2.3.2)
Requirement already satisfied: diskcache>=5.6.1 in ./.venv/lib/python3.12/site-packages (from llama-cpp-python) (5.6.3)
Requirement already satisfied: jinja2>=2.11.3 in ./.venv/lib/python3.12/site-packages (from llama-cpp-python) (3.1.6)
Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)
Building wheels for collected packages: llama-cpp-python
  Building wheel for llama-cpp-python (pyproject.toml): started
  Running command Building wheel for llama-cpp-python (pyproject.toml)
  *** scikit-build-core 0.11.6 using CMake 3.28.3 (wheel)
  *** Configuring CMake...
  loading initial cache file /tmp/tmpgwj49zqh/build/CMakeInit.txt
  -- The C compiler identification is GNU 13.3.0
  -- The CXX compiler identification is GNU 13.3.0
  -- Detecting C compiler ABI info
  -- Detecting C compiler ABI info - done
  -- Check for working C compiler: /usr/bin/x86_64-linux-gnu-gcc - skipped
  -- Detecting C compile features
  -- Detecting C compile features - done
  -- Detecting CXX compiler ABI info
  -- Detecting CXX compiler ABI info - done
  -- Check for working CXX compiler: /usr/bin/x86_64-linux-gnu-g++ - skipped
  -- Detecting CXX compile features
  -- Detecting CXX compile features - done
  CMAKE_BUILD_TYPE=Release
  -- Found Git: /usr/bin/git (found version "2.43.0")
  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD
  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
  -- Found Threads: TRUE
  -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF
  -- CMAKE_SYSTEM_PROCESSOR: x86_64
  -- GGML_SYSTEM_ARCH: x86
  -- Including CPU backend
  -- Found OpenMP_C: -fopenmp (found version "4.5")
  -- Found OpenMP_CXX: -fopenmp (found version "4.5")
  -- Found OpenMP: TRUE (found version "4.5")
  -- x86 detected
  -- Adding CPU backend variant ggml-cpu: -march=native
  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version "12.9.86")
  -- CUDA Toolkit found
  -- Using CUDA architectures: native
  -- The CUDA compiler identification is NVIDIA 12.9.86
  -- Detecting CUDA compiler ABI info
  -- Detecting CUDA compiler ABI info - done
  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped
  -- Detecting CUDA compile features
  -- Detecting CUDA compile features - done
  -- CUDA host compiler is GNU 13.3.0
  -- Including CUDA backend
  -- ggml version: 0.0.1
  -- ggml commit:  4227c9b
  CMake Warning (dev) at CMakeLists.txt:13 (install):
    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.
  Call Stack (most recent call first):
    CMakeLists.txt:108 (llama_cpp_python_install_target)
  This warning is for project developers.  Use -Wno-dev to suppress it.

  CMake Warning (dev) at CMakeLists.txt:21 (install):
    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.
  Call Stack (most recent call first):
    CMakeLists.txt:108 (llama_cpp_python_install_target)
  This warning is for project developers.  Use -Wno-dev to suppress it.

  CMake Warning (dev) at CMakeLists.txt:13 (install):
    Target ggml has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.
  Call Stack (most recent call first):
    CMakeLists.txt:109 (llama_cpp_python_install_target)
  This warning is for project developers.  Use -Wno-dev to suppress it.

  CMake Warning (dev) at CMakeLists.txt:21 (install):
    Target ggml has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.
  Call Stack (most recent call first):
    CMakeLists.txt:109 (llama_cpp_python_install_target)
  This warning is for project developers.  Use -Wno-dev to suppress it.

  CMake Warning (dev) at CMakeLists.txt:13 (install):
    Target mtmd has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.
  Call Stack (most recent call first):
    CMakeLists.txt:162 (llama_cpp_python_install_target)
  This warning is for project developers.  Use -Wno-dev to suppress it.

  CMake Warning (dev) at CMakeLists.txt:21 (install):
    Target mtmd has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.
  Call Stack (most recent call first):
    CMakeLists.txt:162 (llama_cpp_python_install_target)
  This warning is for project developers.  Use -Wno-dev to suppress it.

  -- Configuring done (4.8s)
  -- Generating done (0.0s)
  -- Build files have been written to: /tmp/tmpgwj49zqh/build
  *** Building project with Ninja...
  Change Dir: '/tmp/tmpgwj49zqh/build'

  Run Build Command(s): ninja -v
  [1/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu/hbm.cpp
  [2/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BUILD -DGGML_COMMIT=\"4227c9b\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.0.1\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-threading.cpp
  [3/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BUILD -DGGML_COMMIT=\"4227c9b\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.0.1\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml.cpp
  [4/183] /usr/bin/x86_64-linux-gnu-gcc -DGGML_BUILD -DGGML_COMMIT=\"4227c9b\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.0.1\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-alloc.c
  [5/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu/traits.cpp
  [6/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu/amx/amx.cpp
  [7/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu/amx/mmq.cpp
  [8/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu/vec.cpp
  [9/183] /usr/bin/x86_64-linux-gnu-gcc -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu/arch/x86/quants.c
  [10/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.cpp
  [11/183] /usr/bin/x86_64-linux-gnu-gcc -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.c
  [12/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BUILD -DGGML_COMMIT=\"4227c9b\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.0.1\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-backend.cpp
  [13/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BUILD -DGGML_COMMIT=\"4227c9b\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.0.1\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-opt.cpp
  [14/183] /usr/bin/x86_64-linux-gnu-gcc -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu/quants.c
  [15/183] /usr/bin/x86_64-linux-gnu-gcc -DGGML_BUILD -DGGML_COMMIT=\"4227c9b\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.0.1\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml.c
  [16/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu/unary-ops.cpp
  [17/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu/binary-ops.cpp
  [18/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu/repack.cpp
  [19/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BUILD -DGGML_COMMIT=\"4227c9b\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.0.1\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/gguf.cpp
  [20/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/count-equal.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o
  [21/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/argmax.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o
  [22/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/add-id.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o
  [23/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/argsort.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o
  [24/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/acc.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o
  [25/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/clamp.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o
  [26/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/arange.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o
  [27/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/conv2d-transpose.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o
  [28/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/conv2d-dw.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o
  [29/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/conv-transpose-1d.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o
  [30/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/concat.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o
  [31/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/diagmask.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o
  [32/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/cross-entropy-loss.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o
  [33/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/im2col.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o
  [34/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/fattn.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o
  [35/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu/llamafile/sgemm.cpp
  [36/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/cpy.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o
  [37/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/gla.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o
  [38/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu/arch/x86/repack.cpp
  [39/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f32.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o
  [40/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o
  [41/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o
  [42/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o
  [43/183] /usr/bin/x86_64-linux-gnu-gcc -DGGML_BUILD -DGGML_COMMIT=\"4227c9b\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.0.1\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-quants.c
  [44/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/binbcast.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o
  [45/183] : && /usr/bin/x86_64-linux-gnu-g++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libggml-base.so -o bin/libggml-base.so vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o  -Wl,-rpath,"\$ORIGIN"  -lm && :
  [46/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/mmq.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o
  [47/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/opt-step-adamw.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o
  [48/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/out-prod.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o
  [49/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/roll.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o
  [50/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/pad.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o
  [51/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/scale.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o
  [52/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cpu/ops.cpp
  [53/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/quantize.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o
  [54/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/opt-step-sgd.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o
  [55/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/pool2d.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o
  [56/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/softcap.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o
  [57/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/norm.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o
  [58/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/ssm-conv.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o
  [59/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o
  [60/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o
  [61/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/sumrows.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o
  [62/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/tsembd.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o
  [63/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/fattn-wmma-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o
  [64/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/upscale.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o
  [65/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/rope.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o
  [66/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/softmax.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o
  [67/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/unary.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o
  [68/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/wkv.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o
  [69/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/mean.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o
  [70/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o
  [71/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o
  [72/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/sum.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o
  [73/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o
  [74/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/ssm-scan.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o
  [75/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o
  [76/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o
  [77/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o
  [78/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o
  [79/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o
  [80/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o
  [81/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o
  [82/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o
  [83/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o
  [84/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o
  [85/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o
  [86/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o
  [87/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvf.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvf.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/mmvf.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvf.cu.o
  [88/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o
  [89/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o
  [90/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o
  [91/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o
  [92/183] : && /usr/bin/x86_64-linux-gnu-g++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libggml-cpu.so -o bin/libggml-cpu.so vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o  -Wl,-rpath,"\$ORIGIN"  bin/libggml-base.so  /usr/lib/gcc/x86_64-linux-gnu/13/libgomp.so  /usr/lib/x86_64-linux-gnu/libpthread.a && :
  [93/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o
  [94/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/mmvq.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o
  [95/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-backend-reg.cpp
  [96/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o
  [97/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o
  [98/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-cparams.cpp
  [99/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o
  [100/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama.cpp
  [101/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-hparams.cpp
  [102/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-adapter.cpp
  [103/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-impl.cpp
  [104/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-io.cpp
  [105/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-batch.cpp
  [106/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o
  [107/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-memory.cpp
  [108/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-arch.cpp
  [109/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o
  [110/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-graph.cpp
  [111/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-mmap.cpp
  [112/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-context.cpp
  [113/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-chat.cpp
  [114/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o
  [115/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o
  [116/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-kv-cache-unified-iswa.cpp
  [117/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-model-saver.cpp
  [118/183] /usr/bin/x86_64-linux-gnu-g++   -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -c /tmp/tmpgwj49zqh/build/vendor/llama.cpp/common/build-info.cpp
  [119/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-memory-hybrid.cpp
  [120/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o
  [121/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-memory-recurrent.cpp
  [122/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/unicode-data.cpp
  [123/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o
  [124/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/console.cpp
  [125/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-kv-cache-unified.cpp
  [126/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-model-loader.cpp
  [127/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/llguidance.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/llguidance.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/llguidance.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/llguidance.cpp
  [128/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/log.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/log.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/log.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/log.cpp
  [129/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-grammar.cpp
  [130/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/chat-parser.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/chat-parser.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/chat-parser.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/chat-parser.cpp
  [131/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/ngram-cache.cpp
  [132/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/sampling.cpp
  [133/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/speculative.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/speculative.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/speculative.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/speculative.cpp
  [134/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/json-partial.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/json-partial.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/json-partial.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/json-partial.cpp
  [135/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-vocab.cpp
  [136/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-sampling.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-sampling.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-sampling.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-sampling.cpp
  [137/183] /usr/bin/x86_64-linux-gnu-g++   -O3 -DNDEBUG -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/deprecation-warning.cpp
  [138/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xxs.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o
  [139/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dmtmd_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/../.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/../../vendor -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/mtmd-audio.cpp
  [140/183] /usr/bin/x86_64-linux-gnu-g++   -O3 -DNDEBUG -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/deprecation-warning.cpp
  [141/183] /usr/bin/x86_64-linux-gnu-g++   -O3 -DNDEBUG -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/deprecation-warning.cpp
  [142/183] /usr/bin/x86_64-linux-gnu-g++   -O3 -DNDEBUG -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/deprecation-warning.cpp
  [143/183] : && /usr/bin/x86_64-linux-gnu-g++ -O3 -DNDEBUG  vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o -o vendor/llama.cpp/tools/mtmd/llama-llava-cli   && :
  [144/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/common.cpp
  [145/183] : && /usr/bin/x86_64-linux-gnu-g++ -O3 -DNDEBUG  vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o -o vendor/llama.cpp/tools/mtmd/llama-minicpmv-cli   && :
  [146/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-quant.cpp
  [147/183] : && /usr/bin/x86_64-linux-gnu-g++ -O3 -DNDEBUG  vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o -o vendor/llama.cpp/tools/mtmd/llama-gemma3-cli   && :
  [148/183] : && /usr/bin/x86_64-linux-gnu-g++ -O3 -DNDEBUG  vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o -o vendor/llama.cpp/tools/mtmd/llama-qwen2vl-cli   && :
  [149/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dmtmd_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/../.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/../../vendor -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/mtmd.cpp
  [150/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_xxs.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o
  [151/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/unicode.cpp
  [152/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/include -O3 -DNDEBUG -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/mtmd-cli.cpp
  [153/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_s.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o
  [154/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/regex-partial.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/regex-partial.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/regex-partial.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/regex-partial.cpp
  [155/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq1_s.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o
  [156/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dmtmd_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/../.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/../../vendor -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/clip.cpp
  [157/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/json-schema-to-grammar.cpp
  [158/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmf.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmf.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/mmf.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmf.cu.o
  [159/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_xs.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o
  [160/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o
  [161/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_s.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o
  [162/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xs.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o
  [163/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_nl.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o
  [164/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-mxfp4.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-mxfp4.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-mxfp4.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-mxfp4.cu.o
  [165/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/arg.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/arg.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/arg.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/arg.cpp
  [166/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o
  [167/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o
  [168/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dmtmd_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/../.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/../../vendor -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o -MF vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o.d -o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/tools/mtmd/mtmd-helper.cpp
  [169/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_k.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o
  [170/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/llama-model.cpp
  [171/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q3_k.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o
  [172/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q8_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o
  [173/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_k.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o
  [174/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o
  [175/183] /usr/bin/x86_64-linux-gnu-g++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/src/../include -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/chat.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/chat.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/chat.cpp.o -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/common/chat.cpp
  [176/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q6_k.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o
  [177/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o.d -x cu -c /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q2_k.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o
  [178/183] : && /usr/bin/g++ -fPIC  -shared -Wl,-soname,libggml-cuda.so -o bin/libggml-cuda.so vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmf.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvf.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-mxfp4.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o  -Wl,-rpath,"\$ORIGIN"  bin/libggml-base.so  /usr/local/cuda-12.9/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.9/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  /usr/local/cuda-12.9/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda-12.9/targets/x86_64-linux/lib/libculibos.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl -L"/usr/local/cuda/targets/x86_64-linux/lib/stubs" -L"/usr/local/cuda/targets/x86_64-linux/lib" && :
  [179/183] : && /usr/bin/x86_64-linux-gnu-g++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libggml.so -o bin/libggml.so vendor/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o  -Wl,-rpath,"\$ORIGIN"  -ldl  bin/libggml-cpu.so  bin/libggml-cuda.so  bin/libggml-base.so  -Wl,-rpath-link,/usr/local/cuda/targets/x86_64-linux/lib/stubs && :
  [180/183] : && /usr/bin/x86_64-linux-gnu-g++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllama.so -o bin/libllama.so vendor/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-sampling.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o  -Wl,-rpath,"\$ORIGIN"  bin/libggml.so  bin/libggml-cpu.so  bin/libggml-cuda.so  bin/libggml-base.so  -Wl,-rpath-link,/usr/local/cuda/targets/x86_64-linux/lib/stubs && :
  [181/183] : && /usr/bin/x86_64-linux-gnu-g++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libmtmd.so -o vendor/llama.cpp/tools/mtmd/libmtmd.so vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o vendor/llama.cpp/tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o  -Wl,-rpath,"\$ORIGIN"  bin/libllama.so  bin/libggml.so  bin/libggml-cpu.so  bin/libggml-cuda.so  bin/libggml-base.so  -Wl,-rpath-link,/usr/local/cuda/targets/x86_64-linux/lib/stubs && :
  [182/183] : && /usr/bin/cmake -E rm -f vendor/llama.cpp/common/libcommon.a && /usr/bin/x86_64-linux-gnu-ar qc vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/arg.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/chat-parser.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/chat.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/json-partial.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/llguidance.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/log.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/regex-partial.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/speculative.cpp.o && /usr/bin/x86_64-linux-gnu-ranlib vendor/llama.cpp/common/libcommon.a && :
  [183/183] : && /usr/bin/x86_64-linux-gnu-g++ -O3 -DNDEBUG  vendor/llama.cpp/tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o -o vendor/llama.cpp/tools/mtmd/llama-mtmd-cli  -Wl,-rpath,/tmp/tmpgwj49zqh/build/vendor/llama.cpp/tools/mtmd:/tmp/tmpgwj49zqh/build/bin:  vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/tools/mtmd/libmtmd.so  bin/libllama.so  bin/libggml.so  bin/libggml-cpu.so  bin/libggml-cuda.so  bin/libggml-base.so  -Wl,-rpath-link,/usr/local/cuda/targets/x86_64-linux/lib/stubs && :

  *** Installing project into wheel...
  -- Install configuration: "Release"
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/lib/libggml-cpu.so
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/lib/libggml-cuda.so
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/lib/libggml.so
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/include/ggml.h
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/include/ggml-cpu.h
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/include/ggml-alloc.h
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/include/ggml-backend.h
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/include/ggml-blas.h
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/include/ggml-cann.h
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/include/ggml-cpp.h
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/include/ggml-cuda.h
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/include/ggml-opt.h
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/include/ggml-metal.h
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/include/ggml-rpc.h
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/include/ggml-sycl.h
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/include/ggml-vulkan.h
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/include/ggml-webgpu.h
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/include/gguf.h
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/lib/libggml-base.so
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/lib/cmake/ggml/ggml-config.cmake
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/lib/cmake/ggml/ggml-version.cmake
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/lib/libllama.so
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/include/llama.h
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/include/llama-cpp.h
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/lib/cmake/llama/llama-config.cmake
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/lib/cmake/llama/llama-version.cmake
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/bin/convert_hf_to_gguf.py
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/lib/pkgconfig/llama.pc
  -- Installing: /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/llama_cpp/lib/libllama.so
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/llama_cpp/lib/libllama.so
  -- Installing: /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/llama_cpp/lib/libggml.so
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/llama_cpp/lib/libggml.so
  -- Installing: /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/llama_cpp/lib/libggml-base.so
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/llama_cpp/lib/libggml-base.so
  -- Installing: /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/llama_cpp/lib/libggml-cpu.so
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/llama_cpp/lib/libggml-cpu.so
  -- Installing: /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/llama_cpp/lib/libggml-cuda.so
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/llama_cpp/lib/libggml-cuda.so
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/lib/libmtmd.so
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/include/mtmd.h
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/include/mtmd-helper.h
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/bin/llama-mtmd-cli
  -- Set non-toolchain portion of runtime path of "/tmp/tmpgwj49zqh/wheel/platlib/bin/llama-mtmd-cli" to ""
  -- Installing: /tmp/pip-install-t_mqqhg2/llama-cpp-python_fce0ebf6e47542e08bccc33a8a468a77/llama_cpp/lib/libmtmd.so
  -- Installing: /tmp/tmpgwj49zqh/wheel/platlib/llama_cpp/lib/libmtmd.so
  *** Making wheel...
  *** Created llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl
  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'
  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl size=33418241 sha256=690cd0774d0f31ac915e38aa2d634d9c4d72bb08136aa60a9a3d379f807b3c5c
  Stored in directory: /tmp/pip-ephem-wheel-cache-0knr_mgh/wheels/90/82/ab/8784ee3fb99ddb07fd36a679ddbe63122cc07718f6c1eb3be8
Successfully built llama-cpp-python
Installing collected packages: llama-cpp-python
Successfully installed llama-cpp-python-0.3.16
